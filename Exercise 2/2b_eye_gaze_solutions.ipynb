{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jLn86rurBGf"
      },
      "source": [
        "# Build Your Own MLP - Eye Gaze Estimation\n",
        "\n",
        "In the first part of the course you have implemented your own neural network using nothing but python, numpy and some brain power. In this additional part we will:\n",
        "\n",
        "1. Refactor and generalize the code\n",
        "2. Apply the learned concepts to the task of eye-gaze estimation.\n",
        "\n",
        "Eye-gaze information has many applications in Human-Computer Interaction such as improving user experience in everyday tasks, such as [reading](http://gbuscher.com/publications/BuscherBiedert10_readingRegions.pdf), or facilitate [gaze-based interaction](https://perceptual.mpi-inf.mpg.de/files/2014/07/majaranta14_apc.pdf). Eye-gaze also plays a crucial role in assisting users with motor-disabilities and can even be used to infer cognitive state such as cognitive load. Recently, deep-learning based gaze estimation was used for [unsupervised eye contact detection](https://perceptual.mpi-inf.mpg.de/files/2017/05/zhang17_uist.pdf) in everyday scenarios.\n",
        "\n",
        "The eye gaze data we'll be using is based on [UnityEyes](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/), a synthetic eyes dataset. Note that this is a clean and simple dataset as far as eye gaze estimation goes. The real-world task (real images, uncontrolled environmental conditions) is much more challenging and comprehensively solving this is an active area of research that would go beyond the scope of this notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4LMtG5Zn_-q"
      },
      "source": [
        "!if [ ! -f eye_data.h5 ]; then wget -nv https://github.com/jtj21/ComputationalInteraction18/blob/master/Otmar/data/eye_data.h5?raw=true -O eye_data.h5; fi\n",
        "  \n",
        "import h5py\n",
        "from IPython import display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZvNG2wtrcNn"
      },
      "source": [
        "With the necessary data and libraries imported, let's create split our dataset into training, validation and test sets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lnkOoEbV2_Q"
      },
      "source": [
        "# Load in our data\n",
        "with h5py.File('eye_data.h5', 'r') as h5f:\n",
        "\n",
        "  train_x = h5f['train/x_small'][:]\n",
        "  train_y = h5f['train/y'][:]\n",
        "\n",
        "  validation_x = h5f['validation/x_small'][:]\n",
        "  validation_y = h5f['validation/y'][:]\n",
        "\n",
        "  test_x = h5f['test/x_small'][:]\n",
        "  test_y = h5f['test/y'][:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjQ76AJur1Dd"
      },
      "source": [
        "# Task definition and metric\n",
        "\n",
        "We will apply the concepts we previously learned to build a simple neural network for directly regressing the gaze direction from single eye-images. \n",
        "For this purpose we will represent gaze direction by pitch and yaw in radians. \n",
        "\n",
        "Since pitch and yaw angles are difficult to interpret we will also define a more intuitive *angular* error metric based on [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity#Angular_distance_and_similarity). Angular distance in this case, is a single scalar value which would describe how many degrees the eyeball would need to turn to face match the estimated gaze direction.\n",
        "\n",
        "In addition we will define some helper functions to compute the metrics and a method to visualize our results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIqkl90lr43F"
      },
      "source": [
        "def angular_error(X, y):\n",
        "    \"\"\"Calculate angular error (via cosine similarity).\"\"\"\n",
        "\n",
        "    def pitchyaw_to_vector(pitchyaws):\n",
        "        \"\"\"Convert given pitch and yaw angles to unit gaze vectors.\"\"\"\n",
        "        n = pitchyaws.shape[0]\n",
        "        sin = np.sin(pitchyaws)\n",
        "        cos = np.cos(pitchyaws)\n",
        "        out = np.empty((n, 3))\n",
        "        out[:, 0] = np.multiply(cos[:, 0], sin[:, 1])\n",
        "        out[:, 1] = sin[:, 0]\n",
        "        out[:, 2] = np.multiply(cos[:, 0], cos[:, 1])\n",
        "        return out\n",
        "\n",
        "    a = pitchyaw_to_vector(y) \n",
        "    b = pitchyaw_to_vector(X)\n",
        "\n",
        "    ab = np.sum(np.multiply(a, b), axis=1)\n",
        "    a_norm = np.linalg.norm(a, axis=1)\n",
        "    b_norm = np.linalg.norm(b, axis=1)\n",
        "\n",
        "    # Avoid zero-values (to avoid NaNs)\n",
        "    a_norm = np.clip(a_norm, a_min=1e-7, a_max=None)\n",
        "    b_norm = np.clip(b_norm, a_min=1e-7, a_max=None)\n",
        "\n",
        "    similarity = np.divide(ab, np.multiply(a_norm, b_norm))\n",
        "\n",
        "    return np.arccos(similarity) * (180.0 / np.pi)\n",
        "\n",
        "\n",
        "def predict_and_calculate_mean_error(nn, x, y):\n",
        "    \"\"\"Calculate mean error of neural network predictions on given data.\"\"\"\n",
        "    n, _, _ = x.shape\n",
        "    predictions = nn.predict(x.reshape(n, -1)).reshape(-1, 2)\n",
        "    labels = y.reshape(-1, 2)\n",
        "    errors = angular_error(predictions, labels)\n",
        "    return np.mean(errors)\n",
        "\n",
        "\n",
        "def predict_and_visualize(nn, x, y):\n",
        "    \"\"\"Visualize errors of neural network on given data.\"\"\"\n",
        "    %matplotlib inline\n",
        "    nr, nc = 1, 12\n",
        "    n = nr * nc\n",
        "    fig = plt.figure(figsize=(12, 2.))\n",
        "    predictions = nn.predict(x[:n, :].reshape(n, -1))\n",
        "    for i, (image, label, prediction) in enumerate(zip(x[:n], y[:n], predictions)):\n",
        "        plt.subplot(nr, nc, i + 1)\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        error = angular_error(prediction.reshape(1, 2), label.reshape(1, 2))\n",
        "        plt.title('%.1f' % error, color='g' if error < 7.0 else 'r')\n",
        "        plt.gca().get_xaxis().set_visible(False)\n",
        "        plt.gca().get_yaxis().set_visible(False)\n",
        "    plt.tight_layout(pad=0.0)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-695jpZsRID"
      },
      "source": [
        "# Network structure\n",
        "\n",
        "We can now begin defining our neural network, a simple multi-layer perceptron (MLP) with just a single hidden layer. This time we'll refactor the code a little to make it more configurable than in the first part of the tutorial.\n",
        "\n",
        "\n",
        "## Activation function\n",
        "First, we define a new activation function known as **Rectified Linear Unit** or **ReLU** and its derivative.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Og5HRBXUtFd1"
      },
      "source": [
        "def ReLU(x):\n",
        "    \"\"\"Computes the Rectified Linear Unit function.\"\"\"\n",
        "    return x * (x > 0)\n",
        "\n",
        "def ReLU_(x):\n",
        "    \"\"\"Computes the derivative of the ReLU function.\"\"\"\n",
        "    return (x > 0).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h3yJoyHUtLIc"
      },
      "source": [
        "The ReLU and its derivative look like this:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kna6UyztPhE",
        "outputId": "b0de26be-b1b8-4cd5-8185-a3ffad5448e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        }
      },
      "source": [
        "x = np.linspace(-2., 2., num=400)\n",
        "relu = ReLU(x)\n",
        "relu_prime = ReLU_(relu)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(x, relu, label=\"ReLU\")\n",
        "plt.plot(x, relu_prime, label=\"ReLU prime\")\n",
        "plt.xlabel(\"x\")\n",
        "plt.ylabel(\"y\")\n",
        "plt.xlim([-2, 2])\n",
        "plt.ylim([-1, 2])\n",
        "plt.legend(prop={'size' : 16})\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 576x432 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYZjgBzHuCG-"
      },
      "source": [
        "Note how the ReLU activation function only passes through positive input values, rejecting all negative input values. This simple *non-linear* function can be stacked to create highly complex non-linear models. In addition, the very definite nature of its decision results in a so-called *sparsification* of a neural network, yielding quicker training times. That is, neurons are quickly selected to be \"killed off\", leaving just useful neurons active. See [this page](https://cs231n.github.io/neural-networks-1/) for a concise introduction to various activation functions and their pros and cons.\n",
        "\n",
        "We now define our error metric, the mean-squared error as the following:\n",
        "\n",
        "$$MSE\\left(y,\\hat{y}\\right) = \\frac{1}{n} \\sum_{i=0}^n \\left(y_i - \\hat{y}_i\\right)^2$$\n",
        "\n",
        "where there are $n$ output values in the ground-truth output $y$ and predicted output $\\hat y$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJmmeOCYvrXT"
      },
      "source": [
        "def MSE(Y, YH):\n",
        "    \"\"\"Compute elementwise mean square error between two matrices.\"\"\"\n",
        "    return np.mean(np.square(Y - YH))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-i8OJzo3v29C"
      },
      "source": [
        "We implement our neural network as a class, `NNRegressor`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MRPRvDjnpnbl"
      },
      "source": [
        "class NNRegressor:\n",
        "\n",
        "    def __init__(self, n_outputs, n_features, n_hidden_units=30,\n",
        "                 l2_reg=0.0, epochs=500, learning_rate=0.01,\n",
        "                 batch_size=10, random_seed=None):\n",
        "\n",
        "        if random_seed:\n",
        "            np.random.seed(random_seed)\n",
        "        self.n_outputs = n_outputs\n",
        "        self.n_features = n_features\n",
        "        self.n_hidden_units = n_hidden_units\n",
        "        self.w1, self.w2 = self._init_weights()\n",
        "        self.l2_reg = l2_reg\n",
        "        self.epochs = epochs\n",
        "        self.learning_rate = learning_rate\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def _init_weights(self):\n",
        "        # Truncated normal for weights initialization\n",
        "        w1 = np.random.normal(0.0, 0.01, \n",
        "                              size=self.n_hidden_units * (self.n_features + 1))\n",
        "        w1 = np.clip(w1, -0.01, 0.01)\n",
        "        w1 = w1.reshape(self.n_hidden_units, self.n_features + 1)\n",
        "        w1[:, 0] = 1e-5  # Constant bias initialization\n",
        "        w2 = np.random.normal(0.0, 0.01, \n",
        "                              size=self.n_outputs * (self.n_hidden_units + 1))\n",
        "        w2 = np.clip(w2, -0.01, 0.01)\n",
        "        w2 = w2.reshape(self.n_outputs, self.n_hidden_units + 1)\n",
        "        w2[:, 0] = 1e-5  # Constant bias initialization\n",
        "        return w1, w2\n",
        "\n",
        "    def _add_bias_unit(self, X, how='column'):\n",
        "        if how == 'column':\n",
        "            X_new = np.ones((X.shape[0], X.shape[1] + 1))\n",
        "            X_new[:, 1:] = X\n",
        "        elif how == 'row':\n",
        "            X_new = np.ones((X.shape[0] + 1, X.shape[1]))\n",
        "            X_new[1:, :] = X\n",
        "        return X_new\n",
        "\n",
        "    def _error(self, y, output):\n",
        "        return MSE(y, output)\n",
        "\n",
        "    def _backprop_step(self, X, y):\n",
        "        net_hidden, act_hidden, net_out = self._forward(X)\n",
        "        grad1, grad2 = self._backward(X, net_hidden, act_hidden, net_out, y)\n",
        "        \n",
        "        # regularize\n",
        "        grad1[:, 1:] += 2 * self.l2_reg * self.w1[:, 1:]\n",
        "        grad2[:, 1:] += 2 * self.l2_reg * self.w2[:, 1:]\n",
        "        \n",
        "        error = self._error(y, net_out)\n",
        "        return error, grad1, grad2\n",
        "\n",
        "    def predict(self, X):\n",
        "        net_hidden, act_hidden, net_out = self._forward(X)\n",
        "        return net_out\n",
        "\n",
        "    def fit(self, X_train, y_train, X_test, y_test):\n",
        "        training_errors = []\n",
        "        testing_errors = []\n",
        "\n",
        "        self.error_ = []\n",
        "        for i in range(self.epochs):\n",
        "            n_batches = int(X_train.shape[0] / self.batch_size)\n",
        "            X_mb = np.array_split(X_train, n_batches)\n",
        "            y_mb = np.array_split(y_train, n_batches)\n",
        "            \n",
        "            epoch_errors = []\n",
        "\n",
        "            for Xi, yi in zip(X_mb, y_mb):\n",
        "                batch_size = Xi.shape[0]\n",
        "                \n",
        "                # update weights\n",
        "                error, grad1, grad2 = self._backprop_step(Xi.reshape(batch_size, -1), yi)\n",
        "                epoch_errors.append(error)\n",
        "                self.w1 -= (self.learning_rate * grad1)\n",
        "                self.w2 -= (self.learning_rate * grad2)\n",
        "            mean_epoch_errors = np.mean(epoch_errors)\n",
        "            self.error_.append(mean_epoch_errors)\n",
        "\n",
        "            # Evaluate errors and visualize progress\n",
        "            if i % 5 == 0:\n",
        "                batch_train_error = predict_and_calculate_mean_error(self, Xi, yi)\n",
        "                training_errors.append([i + 1, batch_train_error])\n",
        "                if i % 10 == 0:\n",
        "                    mean_test_error = predict_and_calculate_mean_error(self, X_test, y_test)\n",
        "                    testing_errors.append([i + 1, mean_test_error])\n",
        "                    print('Epoch %d> mean test error: %f degrees' % (i + 1, mean_test_error))\n",
        "                    predict_and_visualize(self, X_test, y_test)\n",
        "\n",
        "        # Now plot a graph of error progression\n",
        "        training_errors = np.asarray(training_errors)\n",
        "        testing_errors = np.asarray(testing_errors)\n",
        "        plt.plot(training_errors[:, 0], training_errors[:, 1], 'g-*', label='train')\n",
        "        plt.plot(testing_errors[:, 0], testing_errors[:, 1], 'b-*', label='test')\n",
        "        plt.legend()\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel('Angular Gaze Error')\n",
        "\n",
        "        return self"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpVdCgWKtEPl"
      },
      "source": [
        "The definition of this class is currently missing the `NNRegressor._forward` and `NNRegressor._backward` method definitions.\n",
        "\n",
        "Please fill this in below, being mindful of the dimensions involved. \n",
        "Note that biases can be pre-pended to the weights. Here is a pictorial representation of this idea:\n",
        "\n",
        "![NN with pre-pended bias node](https://i.imgur.com/u7RsiQR.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQEeeNv5KAFJ"
      },
      "source": [
        "def nn_forward_pass(self, X):\n",
        "    \"\"\"Perform a forward pass of input data through this neural network.\n",
        "    \n",
        "    Note: The output of every step of this forward pass must be cached to be \n",
        "          used for the backward-pass. The backward-pass updates the neural \n",
        "          network weight and bias parameters.\n",
        "    \n",
        "    Params:\n",
        "        X: input data of shape (N x F)\n",
        "\n",
        "    Neural Network Weights:\n",
        "        self.w1: weight parameters of shape (H x F+1)\n",
        "        self.w2: weight parameters of shape (O x H+1)\n",
        "        \n",
        "    Legend:\n",
        "        N: Number of input data entries\n",
        "        F: Number of features\n",
        "        H: Number of neurons in hidden layer\n",
        "        O: Number of output values\n",
        "    \"\"\"\n",
        "    ## First step: ReLU(X * W1)\n",
        "    # Adjust input data to be of shape (N x F+1)\n",
        "    net_input_padded = self._add_bias_unit(X, how='column')\n",
        "\n",
        "    # Calculate hidden layer output of shape (N x H)\n",
        "    net_hidden = net_input_padded.dot(self.w1.T)\n",
        "    \n",
        "    # Calculate hidden layer activations of shape (N x H)\n",
        "    act_hidden = ReLU(net_hidden)\n",
        "    \n",
        "    ## Second step: X * W2\n",
        "    # Adjust activations to be of shape (N x H+1)\n",
        "    act_hidden_padded = self._add_bias_unit(act_hidden, how='column')\n",
        "    \n",
        "    # Calculate neural network output of shape (N x O)\n",
        "    net_out = act_hidden_padded.dot(self.w2.T)\n",
        "    \n",
        "    return net_hidden, act_hidden, net_out\n",
        "\n",
        "NNRegressor._forward = nn_forward_pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7ZBG8yfD9Cj"
      },
      "source": [
        "We can now feed our inputs through the neural network to calculate the output.\n",
        "\n",
        "In order to be able to update our weight and bias matrices, we must now calculate gradients with respect to the output predictions (`net_out`) and ground-truth labels (`y`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0QnsSirhD9Ck"
      },
      "source": [
        "def nn_gradient_calculations(self, net_input, net_hidden, act_hidden, net_out, y):\n",
        "    \"\"\"Calculate gradients for a backward pass through this neural network.\n",
        "    \n",
        "    Params:\n",
        "        net_input: input data of shape (N x F)\n",
        "        net_hidden: output of hidden layer (N x H)\n",
        "        act_hidden: activations of hidden layer (N x H)\n",
        "        net_out: output of neural network (N x O)\n",
        "        y: ground-truth labels (N x O)\n",
        "\n",
        "    Neural Network Weights:\n",
        "        self.w1: weight parameters of shape (H x F+1)\n",
        "        self.w2: weight parameters of shape (O x H+1)\n",
        "        \n",
        "    Legend:\n",
        "        N: Number of input data entries\n",
        "        F: Number of features\n",
        "        H: Number of neurons in hidden layer\n",
        "        O: Number of output values\n",
        "    \"\"\"\n",
        "    \n",
        "    # Calculate error residual (N x O)\n",
        "    de_do = 2*(net_out - y)\n",
        "    \n",
        "    #---#\n",
        "    \n",
        "    # Calculate derivative of output w.r.t w2 (N x H+1)\n",
        "    do_dw2 = self._add_bias_unit(act_hidden, how='column')\n",
        "    \n",
        "    # Calculate gradient w.r.t self.w2 (O x H+1)\n",
        "    de_dw2 = de_do.T.dot(do_dw2)\n",
        "    \n",
        "    #---#\n",
        "    \n",
        "    # Calculate derivative of output w.r.t hidden layer activations (O x H+1)\n",
        "    do_da = self.w2\n",
        "    \n",
        "    # Calculate derivative of hidden layer activations w.r.t hidden layer output (N x H+1)\n",
        "    da_dh = ReLU_(self._add_bias_unit(net_hidden, how='column'))\n",
        "    print(da_dh.shape)\n",
        "    # Calculate derivative of hidden layer output w.r.t w1 (N x F+1)\n",
        "    dh_dw1 = self._add_bias_unit(net_input, how='column')\n",
        "    \n",
        "    # Calculate gradient w.r.t self.w1 (H x F+1)\n",
        "    de_dw1 = (de_do.dot(do_da) * da_dh).T.dot(dh_dw1)[1:, :]\n",
        "\n",
        "    return de_dw1, de_dw2\n",
        "\n",
        "NNRegressor._backward = nn_gradient_calculations"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ga36qbB9KLIt"
      },
      "source": [
        "# Training and prediction\n",
        "\n",
        "Let's test if our implementation works.\n",
        "\n",
        "With the following code, you can create a neural network and train it using our eye-gaze data. While training, you will be able to see visually how well the optimization is proceeding via inspecting a select number of eye-images and their associated training error. After the training stops, you will see a graph summary of how your training and test errors progressed.\n",
        "\n",
        "Look out for the following phenomena:\n",
        "\n",
        "* **Overfitting**. When the network learns too specific patterns of the training data, it becomes less capable of generalizing its predictive capabilities to unseen test data. Or in other words, the training error reduces noticably without a noticable decrease in the test error.\n",
        "* **Underfitting**. When the network is learning so little that it performs as well as it does on seen data (training) and unseen data (test).\n",
        "\n",
        "In addition, pay attention to the speed of the training process. Which parameters should be tweaked to allow for quicker training without overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrzyL_tjsGsx"
      },
      "source": [
        "# A neural network should be trained until the training and test\n",
        "# errors plateau, that is, they do not improve any more.\n",
        "epochs = 201\n",
        "\n",
        "# Having more neurons in a network allows for more complex \n",
        "# mappings to be learned between input data and expected outputs.\n",
        "# However, defining the function to be too complex can lead to \n",
        "# overfitting, that is, any function can be learned to memorize\n",
        "# training data.\n",
        "n_hidden_units = 64\n",
        "\n",
        "# Lower batch sizes can cause noisy training error progression,\n",
        "# but sometimes lead to better generalization (less overfitting\n",
        "# to training data)\n",
        "batch_size = 16\n",
        "\n",
        "# A higher learning rate makes training faster, but can cause\n",
        "# overfitting\n",
        "learning_rate = 0.0005\n",
        "\n",
        "# Increase to reduce over-fitting effects\n",
        "l2_regularization_coefficient = 0.0001\n",
        "\n",
        "N_FEATURES = len(train_x[0, :].flatten())\n",
        "N_OUTPUTS = train_y.shape[1]\n",
        "\n",
        "nn = NNRegressor(n_outputs=N_OUTPUTS,\n",
        "                 n_features=N_FEATURES,\n",
        "                 n_hidden_units=n_hidden_units,\n",
        "                 l2_reg=l2_regularization_coefficient,\n",
        "                 epochs=epochs,\n",
        "                 learning_rate=learning_rate,\n",
        "                 batch_size=batch_size,\n",
        "                 random_seed=42)\n",
        "\n",
        "nn.fit(train_x, train_y, test_x, test_y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjRL2KrJD9Cp"
      },
      "source": [
        "# Appendix\n",
        "\n",
        "What we are performing here is called *mini-batch gradient descent*. That is, we take a small sample of data points from our *training data*, and calculate our gradient update. Note that in the XOR exercise we performed *batch gradient descent* based on the full training dataset of 4 entries.\n",
        "\n",
        "What this means is that our method adds an additional hyperparameter to the training procedure in terms of how well or how quickly we can have our neural network learn. The hyperparameter is namely the *batch size*. If batch size is high, each gradient updates takes more computation but the gradient descent becomes more stable (and slow). If the batch size is too low, each gradient update may be so noisy that it becomes difficult to reach the optimum.\n",
        "\n",
        "When learning data-driven models, we often split our available data into training, validation and testing sub-sets. That is, we train our model on the training data, validate the performance of the model being trained occasionally, and finally produce a test error which characterizes the generalization capability of the model. Low batch sizes have shown to improve generalization, so finding a \"sweet spot\" in terms of batch size can be important in some cases.\n",
        "\n",
        "![](https://i.imgur.com/6spauos.png)\n",
        "\n",
        "A more core consideration for the training of neural networks is learning rate. The graphic below should characterize the three scenarios quite well. Note that this refers to the training loss progression, not the test loss progression.\n",
        "\n",
        "![](https://i.imgur.com/QaikS9E.png)\n",
        "\n",
        "Adjusting the learning rate based on training loss progression alone is insufficient as you can see in the following *over-fitting* phenomenon. That is, our model performs increasingly better on training data, but cannot transfer or *generalize* its knowledge to unseen validation or test data. When this occurs, one should (among other actions) consider lowering both learning rate and batch size.\n",
        "\n",
        "![](https://i.imgur.com/QtMc8us.png)"
      ]
    }
  ]
}